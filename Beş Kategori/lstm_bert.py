# -*- coding: utf-8 -*-
"""LSTM BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GY3hEmJ6NzlIPKiBtFX5NsvyH8sKxCqM
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("data/processed.csv")
df.dropna(inplace=True)

X = df["Tweets"].values
y = df["Tag"].values

from imblearn.over_sampling import RandomOverSampler

os = RandomOverSampler()
X = X.reshape(-1,1)
X, y = os.fit_resample(X, y)
X = X.reshape(-1,)

plt.figure(figsize=(8,5))
sns.countplot(y="Tag", data=df, palette="plasma")
plt.show()

from collections import Counter
def Tokenize(column, seq_len):
    corpus = [word for text in column for word in text.split()]
    count_words = Counter(corpus)
    sorted_words = count_words.most_common()
    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}

    text_int = []
    for text in column:
        r = [vocab_to_int[word] for word in text.split()]
        text_int.append(r)
    features = np.zeros((len(text_int), seq_len), dtype = int)
    for i, review in enumerate(text_int):
        if len(review) <= seq_len:
            zeros = list(np.zeros(seq_len - len(review)))
            new = zeros + review
        else:
            new = review[: seq_len]
        features[i, :] = np.array(new)

    return sorted_words, features
df['text_len'] = [len(text.split()) for text in df.Tweets]
df = df[df['text_len'] < df['text_len'].quantile(0.997)]
max_len = np.max(df['text_len'])

vocabulary, tokenized_column = Tokenize(df["Tweets"], max_len)

tokenized_column[10]

keys = []
values = []
for key, value in vocabulary[:30]:
    keys.append(key)
    values.append(value)

df.dropna(subset=['Tweets'], inplace=True)
plt.figure(figsize=(15, 5))
ax = sns.barplot(x=keys, y=values, palette='mako')
plt.title('Top 30 most common words', size=25)
ax.bar_label(ax.containers[0])
plt.ylabel("Words count")
plt.show()

from sklearn.model_selection import train_test_split
import random
import torch
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

from transformers import BertTokenizer, BertModel
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)
max_len = 30

def tokenize_text(text):
    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_len, return_tensors="pt").to(device)
    return inputs

X_train_tokens = [tokenize_text(text) for text in X_train]
X_test_tokens = [tokenize_text(text) for text in X_test]

def get_bert_embeddings(inputs):
    with torch.no_grad():
        outputs = bert_model(**inputs)
    last_hidden_states = outputs.last_hidden_state
    return last_hidden_states.mean(dim=1).cpu().numpy()

X_train_embed = [get_bert_embeddings(inputs) for inputs in X_train_tokens]
X_test_embed = [get_bert_embeddings(inputs) for inputs in X_test_tokens]

import numpy as np
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint

X_train_pad = np.array(X_train_embed)
X_test_pad = np.array(X_test_embed)

input_shape = X_train_pad.shape[1:]

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization

model = Sequential([
    LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),
    Dropout(0.5),
    LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),
    Dropout(0.5),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dropout(0.5),
    BatchNormalization(),
    Dropout(0.5),
    Dense(5, activation='softmax')
])


model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

checkpoint = ModelCheckpoint("lstm_bert.h5", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test), callbacks=[checkpoint])

model = load_model("lstm_bert.h5")
score = model.evaluate(X_test_pad, y_test, verbose=0)
print('Best Test loss:', score[0])
print('Best Test accuracy:', score[1])

from sklearn.metrics import classification_report

loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)

y_pred_probs = model.predict(X_test_pad)

y_pred = np.argmax(y_pred_probs, axis=1)

categories = ['not cyberbullying', 'religion', 'gender', 'ethnicity', 'age']

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=categories))
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import  confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not cyberbullying', 'religion', 'gender', 'ethnicity', 'age'])
disp.plot(cmap=plt.cm.Blues)
plt.title('LSTM with BERT Embeddings\nConfusion Matrix')
plt.show()