# -*- coding: utf-8 -*-
"""LSTM word2vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lzESV1lD1ihq7Rbk1HIElFUuLLFylkPc
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("data/processed_2.csv")
df.dropna(inplace=True)

X = df["Tweets"].values
y = df["Tag"].values

from imblearn.over_sampling import RandomOverSampler

os = RandomOverSampler()
X = X.reshape(-1,1)
X, y = os.fit_resample(X, y)
X = X.reshape(-1,)

plt.figure(figsize=(8,5))
sns.countplot(y="Tag", data=df, palette="plasma")
plt.show()

from collections import Counter
def Tokenize(column, seq_len):
    corpus = [word for text in column for word in text.split()]
    count_words = Counter(corpus)
    sorted_words = count_words.most_common()
    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}

    text_int = []
    for text in column:
        r = [vocab_to_int[word] for word in text.split()]
        text_int.append(r)
    features = np.zeros((len(text_int), seq_len), dtype = int)
    for i, review in enumerate(text_int):
        if len(review) <= seq_len:
            zeros = list(np.zeros(seq_len - len(review)))
            new = zeros + review
        else:
            new = review[: seq_len]
        features[i, :] = np.array(new)

    return sorted_words, features
df['text_len'] = [len(text.split()) for text in df.Tweets]
df = df[df['text_len'] < df['text_len'].quantile(0.997)]
max_len = np.max(df['text_len'])

vocabulary, tokenized_column = Tokenize(df["Tweets"], max_len)

tokenized_column[10]

keys = []
values = []
for key, value in vocabulary[:30]:
    keys.append(key)
    values.append(value)

df.dropna(subset=['Tweets'], inplace=True)
plt.figure(figsize=(15, 5))
ax = sns.barplot(x=keys, y=values, palette='mako')
plt.title('Top 30 most common words', size=25)
ax.bar_label(ax.containers[0])
plt.ylabel("Words count")
plt.show()

from sklearn.model_selection import train_test_split
import random
import torch
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
Word2vec_train_data = list(map(lambda x: x.split(), X_train))

import gensim
from gensim.models import Word2Vec

model = Word2Vec(Word2vec_train_data, vector_size=100, window=5, min_count=1, workers=4)

word_vector = model.wv['word']

similar_words = model.wv.most_similar('word')

model.save("word2vec_model")

w2v_model = Word2Vec.load("word2vec_model")

import numpy as np
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import ModelCheckpoint

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)

embedding_dim = w2v_model.vector_size

vocab_size = len(tokenizer.word_index) + 1
embedding_matrix = np.zeros((vocab_size, embedding_dim))
for word, i in tokenizer.word_index.items():
    if word.strip() in w2v_model.wv:
        embedding_matrix[i] = w2v_model.wv[word.strip()]
    else:
        embedding_matrix[i] = np.random.normal(size=(embedding_dim,))


model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False))
model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dropout(0.5))
model.add(BatchNormalization())
model.add(Dense(2, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

checkpoint = ModelCheckpoint("best_model_word2vec_lstm.keras", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

model.fit(X_train_pad, y_train, epochs=30, batch_size=64, validation_data=(X_test_pad, y_test), callbacks=[checkpoint])

model = load_model("best_model_word2vec_lstm.keras")
score = model.evaluate(X_test_pad, y_test, verbose=0)
print('Best Test loss:', score[0])
print('Best Test accuracy:', score[1])

from sklearn.metrics import classification_report

loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)

y_pred_probs = model.predict(X_test_pad)

y_pred = np.argmax(y_pred_probs, axis=1)

categories = ["not cyberbullying", "cyberbullying"]

print("Classification Report:")
print(classification_report(y_test, y_pred, target_names=categories))
print("Test Loss:", loss)
print("Test Accuracy:", accuracy)

from sklearn.metrics import  confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['not cyberbullying', 'cyberbullying'])
disp.plot(cmap=plt.cm.Blues)
plt.title('LSTM with Word2Vec Embeddings\nConfusion Matrix')
plt.show()